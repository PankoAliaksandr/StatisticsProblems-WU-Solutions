\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\author{Group 8}
\title{Statistics 1 Unit 2}
\begin{document}
\SweaveOpts{concordance=TRUE}

	\maketitle
	\tableofcontents

\section{Task 15}
\subsection{a)}
From Sylvester's Criterion it is clear that $\alpha$ being the first leading principal minor must be positive. 

Since matrix $B$ is a positive definite square matrix, $x'Bx > 0 $ for all $x$. Let's take $x = (0,x_1...x_n)$. In this case, $x'Bx = y'Ay$, where $y = (x_1...x_n)$ and it must be $> 0$. Since $y'Ay > 0$ for arbitrary $y = (x_1...x_n)$, A is positive definite by definition.


\subsection{b)}
\begin{align*}
C = \begin{pmatrix} D & C^T \\ C & E \end{pmatrix}
=\begin{pmatrix} 1 & 0 \\ CD^{-1} & 1 \end{pmatrix}
\begin{pmatrix} D & 0 \\ 0 & S \end{pmatrix}
\begin{pmatrix} 1 & D^{-1}C^T \\ 0 & 1 \end{pmatrix}
\end{align*}
Substitution $D = L_dL_d^T$ and $S = L_sL_s^t$ gives us the Cholesky factorization:
\begin{align*}
B = \begin{pmatrix} D & C^T \\ C & E \end{pmatrix} = \begin{pmatrix} L_d & 0 \\ CL_d^{-1} & L_s \end{pmatrix}\begin{pmatrix} L_d^t & L_d^{-1}C^T \\ 0 & L_s^t \end{pmatrix} = L_bL_b^t
\end{align*}

In our case: D=$\alpha$, C=a, E=A, $C^t=a^t$
then $L_d$ = $\sqrt{\alpha}$, $L_d^{-1}$ = $\frac{1}{\sqrt{\alpha}}$, $L_d^t$ =$\sqrt{\alpha}$, $D^{-1}$ = $\frac{1}{\alpha}$

\begin{align*}
S = A - \frac{1}{\alpha}aa^t
\end{align*}

\begin{align*}
S = L_sL_s^t - Choleski-Factorsation
\end{align*}
Hence,
\begin{align*}
B = \begin{pmatrix} \alpha & a^T \\ a & A \end{pmatrix} =\begin{pmatrix} \sqrt{\alpha} & 0 \\ \frac{1}{\alpha}a & L_s \end{pmatrix}
\begin{pmatrix} \sqrt{\alpha} & \frac{1}{\alpha}a^T \\ 0 & L_s^T \end{pmatrix}
\end{align*}

\section{Task 16}
\subsection{a)}
Here we use the same idea. Since matrix $B$ is a positive definite square matrix, $x'Bx > 0 $ for all $x$. Let's take $x = (x_1...x_n,0)$. In this case, $x'Bx = y'Ay$, where $y = (x_1...x_n)$ and it must be $> 0$. Since $y'Ay > 0$ for arbitrary $y = (x_1...x_n)$, A is positive definite by definition. For $\alpha$ we can use $x = (0...0,1)$. Applying the same idea, fact that $\alpha >0 $ is easily proved.


\subsection{b)}
The idea of Choleski decomposition is absolutelly the same as in 15(b).
In this case: D=A, $C=a^t$, E=$\alpha$, $C^t=a$

This gives us:
\begin{align*}
B = \begin{pmatrix} A  & a \\ a^T & \alpha \end{pmatrix} =\begin{pmatrix} L_A & 0 \\ a^tL_A^{-1} & \sqrt{\alpha -a^TA^{-1}a} \end{pmatrix}\begin{pmatrix} L_A^T & L_A^{-1} \\ 0 & \sqrt{\alpha -a^TA^{-1}a} \end{pmatrix}
\end{align*}

\section{Task 17}
<<>>=
f17 <- function(k) {
mat_A <- matrix(c(10^(-2*k),1,1,1), 2, 2)
mat_M <- matrix(c(1, -1/(10^(-2*k)), 0,1), 2, 2)
vec_B <- matrix(c(1+(10^(-2*k)), 2), 2, 1)
S <- backsolve((mat_M %*% mat_A), (mat_M %*% vec_B))
S
}
a<-seq(1:10)
sapply(a, f17)
@

When, $\epsilon$ decreases $(k>6)$ we are getting greater error.

\section{Task 18}
<<>>=
matrix_2norm <- function(x){
  norm(x, type="2") 
}

x <- matrix(c(1,2,3,4),2,2)

max(svd(x)$d)
matrix_2norm(x)
@
It is clear that 2-norm is just the maximum value of the singular value decomposition of the matrix.


\section{Task 19}
<<>>=
cond_num <- function(p){
  r <- matrix_2norm(p)
  s <- matrix_2norm(solve(p))
  s * r
}

max(svd(x)$d) / min(svd(x)$d)
cond_num(x)
@
We observe that the condition number of a matrix is the quotient of the lowest and the highest value of the SVD.\\
This follows from: $M = U \Sigma V'$ and $M^{-1} = V \Sigma ^{-1} U'$. The values of the diagonal matrix $\Sigma ^{-1} $ are just the reciprocals of the diagonal values of $\Sigma$. Since the 2-norm is the maximum value of the SVD, the 2-norm of $\Sigma ^{-1}$ has to be 1 over the smallest singular value of the matrix.

\section{Task 20}
Ax=b \\
$
\Delta x=A^{-1} \Delta b \\
\|b\| = \|Ax\| \|x\| \\
\text{Using norm's property:} \\
\|Ax\| \leq \|A\| \|x\|\\
\Rightarrow \|\Delta x\| = \|A^{-1} \Delta b\| \leq \|A^{-1}\| \|\Delta b\| \wedge \|b\| = \|Ax\| \leq \|A\| \|x\| \\
\text{Multiply these two inequalities (the norm} $\textgreater$ \text{0)} \\
\|\Delta x\| \|b\| \leq \|A^{-1}\| \| \Delta b\| \|A\| \|x\| \\
\text{Divide by:} \|b\| \|x\| \text{and get}\\
 \frac{||\Delta x||}{||x||}\leq k(A) \frac{||\Delta b||}{||b||}
$


\section{Task 21}

<<>>=
# Matrix is filled by columns by default
A <- function(epsilon) {
  matrix(data = c(1, 1-epsilon, 1+epsilon, 1),nrow =  2, ncol = 2)}
@
Now, given a small $\epsilon$ we will trying to solve the system.
"normal" solve command, qr.solve, qr.decomposition "by hand" and the SVD method.\\
As suggested we will use sqare root of the machine precision as our $\epsilon$.

<<>>=
epsilon <- unname(sqrt(unlist(.Machine[1])))
epsilon

b <- c(1+epsilon +epsilon^2, 1)
@
First, let's check condition number using task 19 approach:
<<>>=
svdA <- svd(A(epsilon))

cond.nr <- max(svdA$d) / min(svdA$d)
cond.nr
@
As we can see the condition number is very big. This means that small changes of a parameter will dramatically change the solution.

\subsection{Solve}
<<>>=
# solve(A(epsilon), b)
# Error in solve.default(A(epsilon), b) : 
#  system is computationally singular: reciprocal condition number = 5.55112e-17
@
\subsection{QR.Solve}
<<>>=
# qr.solve(A(epsilon), b)
# Error in qr.solve(A(epsilon), b) : singul√§re Matrix 'a' in 'solve'
@
\subsection{SVD}
<<>>=
# svdA <- svd(A(epsilon))
# D <- diag(x=svdA$d)
# D
# a <- (tcrossprod(svdA$u %*% D, svdA$v))
#solve(a,b)
# Error in solve.default(a, b) : 
#  system is computationally singular: reciprocal condition number = 2.77556e-17
@
\subsection{QR + rearrangement}
QR decomposition using rearrangement $Rx=Q'b$ gives result, but with really big error:
<<>>=
AQR <- qr(A(epsilon))
qr.Q(AQR)
backsolve(qr.R(AQR), crossprod(qr.Q(AQR),b))
@
Since the relative error is bounded by $\mathcal{K}(A)\frac{||\Delta b||}{||b||}$,larger $\epsilon$ leads to smaller condition numbers $\mathcal{K}(A)$ and hence more accurate solution.


\section{Task 23}
\subsection{a}
To prove the task just use the formula below:
\begin{align*}
Au = (uv^T)u=u(v^Tu)=(v^Tu)u.
\end{align*}

\subsection{b}
The other eigenvalues of A. Since rank(A) = 1 all other eigenvalues equals are zeros.

\subsection{c}
Let U $\in$ $C_{m\times m}$ be a unitary matrix
so that $Uu = \|u\|_2e1$, and V $\in$ $C_{n\times n}$ be a unitary matrix so that $Vv = \|v\|_2e1$. We can substitute it into uv* shows SVD. $D =  \|u\|_2\|v\|_2e1e1*$
 
\section{Task 26}
From task 19 follows that the ratio is a condition number of out matix. To show the dinamic it is convenient to use a plot: x-axis the dimention, y-axis the condition number.
<<>>=
f26 <- function(n) {
  M <- mat.or.vec(n, n)
  M[upper.tri(M)] <- -1
  diag(M) <- 1
  condition_number <- max(svd(M)$d) / min(svd(M)$d)
  return (condition_number)
}

x <- 3:10
y <- c(rep(0, 8))
for (i in (1:8)) {
  y[i] <- f26(x[i])
}

@
<<fig=TRUE>>=
plot(x,y)
@
\newline
As we can see, condition number exponentially increases with increasing the dimension of matrix.

\section{Task 28}
Singular Valued Decomposition constructs orthonormal bases
for the range and null space of a matrix
The columns of U which correspond to non-zero singular
values of A are an orthonormal set of basis vectors for
the range of A
The columns of V which correspond to zero singular values
form an orthonormal basis for the null space of A

<<>>=
# The columns of output matrix contain orthonormal bases for the range of A
get_bases_for_the_range <- function (A)
{
  A_svd = svd(A)
  
  d = A_svd$d
  u = A_svd$u
  v = A_svd$v
  
  column_indices <- which(d >= .Machine$double.eps)
  
  B = u[,column_indices]
  
  return(B)
}

# The columns of output matrix contain orthonormal bases for the null space of A
get_bases_for_the_null_space <- function (A)
{
  A_svd = svd(A)
  
  d = A_svd$d
  u = A_svd$u
  v = A_svd$v
  
  column_indices <- which(d < .Machine$double.eps)
  
  B = v[,column_indices]
  
  return(B)
}

# Test
A <- matrix (c(3,7,8,7,10,9,8,9,12, 15) , nrow = 5, ncol = 2)

get_bases_for_the_range(A)
get_bases_for_the_null_space(A)
@

\section{Task 31}
Required function is represented below:
<<>>=
library(matrixcalc)
Duplication <- function (n) {
  # Arbitrary matrix
  A <- matrix((1:n ^ 2), n, n)
  # Make it symmetric
  A[lower.tri(A)] = t(A)[lower.tri(A)]
  vec(A)
  vech(A)
  D <- mat.or.vec(length(vec(A)), length(vech(A)))
  for (i in 1:n ^ 2) {
    col_num <- match(vec(A), vech(A))
    row_num <- 1:n ^ 2
    D[row_num[i], col_num[i]] <- 1
  }
  return(D)
}

Duplication(2)
@


\section{Task 32}
Let's compute Singular Value Decomposition of the duplication matrix $D_{n}$ with $ n=2$ and $ n=3$:
<<>>=
svd(Duplication(2))
svd(Duplication(3))
@
It can be seen that if $n=2$, then last $2$ diagonal elements of matrix $D$, which are singular values, are equal to $1$.
If $n=3$, then the last 3 diagonal elements of matrix $D$ are equal to $1$. Other elements are $1.414214$ what is just the square root of $2$.


\section{Task 33}
Function returning the elimination matrix $L_{n}$ for given $n$ and example of its application are as follows:
<<>>=
library(matrixcalc)
Elimination <- function (n) {
A <- matrix((1:n^2),n,n)
vec(A)
vech(A)
D <- mat.or.vec(length(vech(A)),length(vec(A)))
for (i in 1:n^2) {
col_num <-match(vech(A), vec(A))
row_num <- 1:(n*(n+1)/2)
D[row_num[i], col_num[i]] <- 1}
return(D)}

Elimination(2)
@


\section{Task 34}
Let's compute Singular Value Decomposition of the elimination matrix $L_{n}$ for $n=2$:
<<>>=
svd(Elimination(2))
@
It can be seen that matrix $U$ is an identity matrix and diagonal matrix $D$ is an identity matrix too. In addition, the elimination matrix itself equals to $V^{t}$ matrix.


\section{Task 35}
Function returning the commutation matrix $K_{mn}$ for given $m$ and $n$ and example of its application are as follows:
<<>>=
library(matrixcalc)
commutation_matrix <- function (r, c = r) 
{
  H <- H.matrices(r, c)
  p <- r * c
  K <- matrix(0, nrow = p, ncol = p)
  for (i in 1:r) {
    for (j in 1:c) {
      Hij <- H[[i]][[j]]
      K <- K + (Hij %x% t(Hij))
    }
  }
  return(K)
}

k <- commutation_matrix(3,2)
k
@

\section{Task 36}
The singular values of the commutation matrix are:
<<>>=
svd(k)$d
@

\section{Task 37}
The Moore-Penrose inverse can be expressed in terms of SVD, $A= UDV'$, since $A^+= VD^{-1}U'$\\ 
where each element in $D^{-1}$ is taken as a reciprocal of corresponding element in matrix $D$, if it is greater, than given tolerance, or $0$ otherwise.
<<>>=
f37 <- function(A, tol = 1e-10){
  SVD <- svd(A)
  D <- NULL
  for(i in 1:length(SVD$d)){
    if(SVD$d[i] < tol){
      SVD$d[i] <- 0
      D <- c(D,as.numeric(SVD$d[i]))
    }
    else{
      D <- c(D,as.numeric(1/(SVD$d[i])))
    }
  }
  SVD$v %*% diag(D) 
  A_plus <- crossprod(t(crossprod(t(SVD$v), diag(D))), t(SVD$u))
  return(A_plus)
}
@


\section{Task 38}
Let's use the property that the trace is invariant under cyclic permutations when inner matrix is square.\\
<<>>=
wcptrace <- function(A, w) {
  if(dim(A)[1] == dim(A)[2]) {
    a <- numeric(dim(A)[1])
    for(i in 1:dim(A)[1]){
      a[i] <- sum(A[i, ]^2)
    }
    return(sum(a * w))
  } else {
    cat("The input matrix is not square, therefore there is not much to improve in terms of efficiency). At least one matrix multiplication has to be done")
    sum(diag(t(A) %*% diag(w) %*% A))
  }
}
  
mat <- matrix(data = rexp(200, rate = 10), nrow = 100, ncol = 100)
w <- sample(1:100)
system.time( replicate(10000, wcptrace(mat, w)))
system.time( replicate(10000, sum(diag(t(mat) %*% diag(w) %*% mat))))
@

\section{Task 39}
<<>>=
#Using Cholesky Decomposition is more efficient for our task

dmvnorm2 <- function (x, m, V) {
  mat_chol <- chol(V)
  delta <- x - m
  y <-det(mat_chol)^2 * (2*pi)^nrow(mat_chol)
  e <- exp(-t(delta) %*% solve(mat_chol) %*% t(solve(mat_chol)) %*% delta / 2)
  diag(y^(-1/2) * e)
}

#TEST
library(mvtnorm)
M <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5,0.5,1),nrow=3)
dmvnorm(1:3, 1:3, M)
#Our code test
dmvnorm2(1:3, 1:3, M)
@

\end{document}